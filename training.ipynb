{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "# Loading important libraries that will help us regress the iMBD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that we have the necessary libraries, we need to transform the data into \n",
    "a pandas dataframe that way we can understand better what information is being stored with\n",
    "in the dataset. We will also need to split the data into training and testing data that way\n",
    "we can get a baseline for how our first model will perform that way we can tune it later on when\n",
    "we peform a 5 fold cross validation.\n",
    "\n",
    "I have the data in a csv file that I have downloaded from Kaggle that I have renamed movie_reviews.csv\n",
    "in this directory. Using the read_csv method from pandas, we can easily load in the data and view it.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(\"data.csv\", names=[\"Review\", \"Sentiment\"]) # loading in the data\n",
    "df = df.iloc[1:] # removing the first row because it is just the column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Sentiment\n",
       "1  One of the other reviewers has mentioned that ...  positive\n",
       "2  A wonderful little production. <br /><br />The...  positive\n",
       "3  I thought this was a wonderful way to spend ti...  positive\n",
       "4  Basically there's a family where a little boy ...  negative\n",
       "5  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now that we have a representation of the data, let's visual what it looks in the dataframe.\n",
    "Lets understand some basic information like the feature(s) (Review) and the target(s) (Sentiment).\n",
    "We can also get some more information by using the describe method from pandas to understand anything\n",
    "that might be interesting about the data or missing values.\n",
    "\"\"\"\n",
    "\n",
    "df.head() # viewing the first 5 rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Review Sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Off rip, we can see that we have reviews as the one feature which we will need to later break up into\n",
    "multiple feature values like words/characters that hold a heuristic value and ignoring some as well. We also \n",
    "have a sentiment which is simply just the label as if the movie was good or bad. Movies that are good are labeled \n",
    "as positive and have a rating above 6 and movies that are bad are labeled as negative and have a rating of 6 or below.\n",
    "\n",
    "Also looking at the description of the data, I notice that we have 50000 reviews although some reveiws are not entirely\n",
    "unique. This could mean a multitude of things like there being NA values or duplicates which may need to be handled.\n",
    "Another thing I notice is that the Reviews has a frequency of 5. I am assuming this means that there's a lot of reviews\n",
    "that share the same rating but I am honestly not too sure exactly what this 5 could represent.\n",
    "\"\"\"\n",
    "\n",
    "df.describe() # viewing the summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\"\"\"\n",
    "For this particular problem we have no real values that we can use to train a logistic regression model.\n",
    "You might ask, what are we exactly supposed to do with the words we have been given? Well we can evaluate the\n",
    "words based on their frequencies and how important they are as words. We can do this by creating a bag of words\n",
    "model. This bag of words model is simply a dictionary of words that we want to count as well as how many times\n",
    "they occur in the reviews. Some words might hold values of zeroes and some words may hold many values for their\n",
    "frequency but the point is, we can turn our one feature (reviews) into multiple continuous features using our\n",
    "bag of words model.\n",
    "\n",
    "Another thing we must consider when extracting information from the reviews is unwanted words, symbols, formating, etc.\n",
    "I notice in one particular review that their are html break tags in the review. We need to find a way to parse all of our\n",
    "reviews and make sure they only contain \"Rich\" information that can be used as a feature and not useless information like\n",
    "words: \"the\", \"and\", \"a\", etc.\n",
    "\n",
    "Using sci-kit learn's CountVectorizer class we can easily create a bag of words model from the reviews. Before we do\n",
    "I am going to apply it on a sample to get familiar with the bag of words model and using it.\n",
    "\"\"\"\n",
    "\n",
    "samples = np.array(df.sample(n=10).drop(columns=[\"Sentiment\"])) # sampling 100 reviews from the data (removing the sentiment column)\n",
    "\n",
    "counts = CountVectorizer() \n",
    "bag = counts.fit_transform(samples.ravel()) # fitting the model to the reviews (getting frequencies of words in reviews as well as total words in all reviews)\n",
    "\n",
    "dictionary = counts.vocabulary_\n",
    "\n",
    "\n",
    "\n",
    "bag = bag.toarray() # converting the bag of words model to a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 561\n",
      "good: 229\n",
      "bad: 53\n",
      "Example review and its word frequencies:\n",
      "[ 0  1  0  0  0  0  0  3  1  0  2  1  0  1  1  1  0  1  0  1  0  3  3  0\n",
      "  0  1  0  0  1  2  0 12  0  0  1  0  0  1  1  3  0  3  0  1  3  0  1  1\n",
      "  0  0  0  0  0  0  0  0  0  0  1  1  4  0  1  0  0  1  0  0  0  3  0  0\n",
      "  0  0  0 14  0  0  1  0  0  0  0  4  3  0  1  0  0  0  0  0  0  1  0  1\n",
      "  2  1  1  1  2  1  1  0  0  4  0  0  0  0  1  0  0  1  0  1  0  0  0  0\n",
      "  0  2  1  0  0  0  0  1  0  1  0  0  1  1  1  1  0  0  1  0  2  1  1  0\n",
      "  1  0  0  1  0  0  2  1  0  0  0  0  0  0  0  0  1  0  0  0  0  1  1  0\n",
      "  0  2  0  0  0  0  0  0  0  0  0  1  0  0  0  1  1  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  1  0  1  0  1  5  0  1  0  0  0  0  0  2  0  0  1  0\n",
      " 12  5  2  0  0  0  0  0  0  1  0  1  0  0  0  0  0  7  2  0  0  2  0  2\n",
      "  0  0  1  0  7  0  6  0  1  2  1  0  0  1  6  0  0  2  0  1  1  0  1  0\n",
      "  2  1  0  0  1  1  0  0  1  0  0  1 12  0  2  1  1  0  0  1  0  1  0  0\n",
      "  1  1  2  1  0  7  1  0  2  0  0  2  0  0  0  0  0  0  0  0  0  2  2  0\n",
      "  0  4  0  1  0  1  0  0  0  4  2  0  1  1  0  0  0  0  0  0  0  0  1  0\n",
      "  0  0  0  0  1  1  0  0  0  1  1  0  0  0  0  1  1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  1  3  0  0  0  0  1  1  0  0  1  8  1  0  0  1  0  0  2\n",
      "  1  0  0  3  3  0  0  0  0  0 28  0  0  0  1  3  1  0  0  2  1  1  2  0\n",
      "  0  0  1  0  0  0  0  1  1  0  0  1  0  2  0  0  0  1  0  0  0  1  2  0\n",
      "  1  0  1  0  1  0  1  0  0  0  2  0  1  1  0  1  0  1  1  0  0  0  0  0\n",
      "  1  0  0  1  0  1  0  1  2  0  0  1  0  0  0  1  4  0  1  0  0  2  1  1\n",
      "  1  0  0  1  1  1  0  2  0  0  1  0  0  0  1  0  0  0  0  0  0  0  3  0\n",
      "  0  0  0  0  0  1  0  0  0  0  1  1  0  0  1  1  0  1  1  0  0  0  0  1\n",
      "  0  0  2  0  0  1  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  1  0\n",
      "  0  0  0  0  0  1  0  2  2 59  0  1  1  0  0  1  0  0  0  1  5  0  0  1\n",
      "  2  0  1  0  1  2  0 17  0  1  0  1  0  2  0  0  1  2  0  1  0  0  0  0\n",
      "  0  2  0  0  2  1  0  0  0  0  0  0  0  2  6  1  0  0  0  1  1  0  0  4\n",
      "  1  0  2  1  0  1  0  0  0  1  3  0  0  0  0  0  0  0  1  1  0  1  1  0\n",
      "  2  0  1  0  0  0  0  5  1  0  0  0  0  0  0  0  0  2  1  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the code above we can now take a look at some of the words that we have extracted from the reviews. We can see\n",
    "things such as their indicies relative to the sequences in the counts vocabulary_ attribute and we can also see the tokenization of words\n",
    "in each review. The indicies of the review sequence represent a particular word in the counts vocabulary_ attribute. The\n",
    "value with in that index is the frequency of that word in the review. The indicies of the sequence remain consistent\n",
    "across all reviews which will help us later on when deciding word importance.\n",
    "\n",
    "One thing I want to consider that we will implement later on is how we will value these words or how we will give them\n",
    "some type of inherent value that makes them more important towards good or bad reviews over others. Let's take a look\n",
    "at the word frequency of the word \"the\" in the reviews vs good and bad reviews. We can't make the decision based off the word\n",
    "frequency alone because of words like \"the\" that are common in good and bad reviews, so we will need a method of determining\n",
    "the relative importance of the word.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"the: {dictionary['the']}\\ngood: {dictionary['good']}\\nbad: {dictionary['bad']}\") # index of good and bad relative to all sequences\n",
    "\n",
    "print(f\"Example review and its word frequencies:\\n{bag[0]}\") # example review and its word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A token we might be looking for, for a movie review and its index location in the sequence array: blood of - 184\n",
      "An example sequence of the 2 word ngram model: [1 0 0 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The example above is one example sequence from the unigram model we created. We get the prefix \"uni\" for one meaning we have one word\n",
    "for every token in the sequence of a single review. We can change a parameter when initializing the CountVectorizer class\n",
    "to use a n-gram model where we can make a single token be 2, 3, or 4 words etc. This can help enrich our bag of words model even more by\n",
    "certain phrases that are more common in good and bad reviews, but excessive use of n-grams can lead to overfitting in which a good or bad\n",
    "review must contain some super unique phrase or ordering of words to be classified as a positive or negative review which we don't want.\n",
    "\n",
    "Lets do a quick demo with this feature to get familiar with the the ngram model.\n",
    "\"\"\"\n",
    "\n",
    "counts = CountVectorizer(ngram_range=(2,2)) # using a ngram model with 2 words for each token\n",
    "bag = counts.fit_transform(samples.ravel()) # fitting the model to the reviews (getting frequencies of words in reviews as well as total words in all reviews)\n",
    "\n",
    "dictionary = counts.vocabulary_\n",
    "\n",
    "print(f\"A token we might be looking for, for a movie review and its index location in the sequence array: {list(dictionary.keys())[0]} - {dictionary[list(dictionary.keys())[0]]}\") # the first token in the dictionary\n",
    "print(f\"An example sequence of the 2 word ngram model: {bag.toarray()[8]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Now that we have our ngram model, we need a way to evulate each word or phrase (depeneding on what we do) to determine its value.\n",
    "We can do this by using the tf-idf model which is a combination of the bag of words model and the inverse document frequency model.\n",
    "Essentially, we are multiplying the frequency of a token in all documents times the log applied to the total number of documents\n",
    "divided by the sum of 1 and the amount of documents where the token appears. This equation essentially \"weights\" tokens based on \n",
    "their relative importance accross the documents of good and bad reviews. There is an alternative equation that just is the same as\n",
    "above although it sums 1 along with the number of total documents instead of just the total documents in the numerator and then adds\n",
    "1 to the idf (to give non-zero values a value). We will use this one over the prior for the reasons mentioned previously.\n",
    "\n",
    "Using the formulas given and a little bit of basic programming, I'm going to make a function that will process an entire sample\n",
    "set into a tf-idf model normalized. After this has been implemented, there is one last thing we need to consider before creating\n",
    "training/testing data and doing cross validation which is filtering. What I mean by this is removing things like stop words (words that\n",
    "provide little to no information across either class) and removing some symbols or HTML tags that I talked above before.\n",
    "\"\"\"\n",
    "\n",
    "# l2 normalization\n",
    "def normalize(x):\n",
    "    x = np.array(x)\n",
    "    return x / np.sqrt(np.sum(x**2))\n",
    "\n",
    "# calculates document frequencies for entire samples according to dictionary of tokens\n",
    "def doc_freq(samples, dictionary):\n",
    "\n",
    "    doc_freqs = dict.fromkeys(list(dictionary.keys()), 0)\n",
    "    \n",
    "    for token in doc_freqs:\n",
    "\n",
    "        doc_freqs[token] = np.sum([1 for sample in samples if token in sample.lower()])\n",
    "    \n",
    "    return doc_freqs\n",
    "\n",
    "# function that takes an entire sample set and gets the l2 normalized tf * idf for each sequence\n",
    "def tfidf(samples):\n",
    "\n",
    "    counts = CountVectorizer()\n",
    "    bag = np.array(counts.fit_transform(samples.ravel()).toarray()).astype(float) # creating bag of words\n",
    "    dictionary = counts.vocabulary_ # getting the dictionary of words and their index location for each sequence\n",
    "    doc_freqs = doc_freq(samples, dictionary) # getting the document frequency of all tokens\n",
    "    dictionary = {v: k for k, v in dictionary.items()} # reversing the dictionary so we can use the index location to get the word\n",
    "\n",
    "    N = len(bag) # total number of documents\n",
    "\n",
    "    # iterate every sequence\n",
    "    for idx, sequence in enumerate(bag):\n",
    "        \n",
    "        if idx % 5000 == 0:\n",
    "            print(f\"{idx} documents processed / {N}\")\n",
    "\n",
    "        sequence = np.array(sequence) # converting to numpy array\n",
    "        \n",
    "        # iterate each token in the sequence\n",
    "        sequence = [tf * (1 + np.log((N + 1) / (1 + doc_freqs[dictionary[i]]))) for i, tf in enumerate(sequence)] # calculating tf-idf\n",
    "            \n",
    "        bag[idx] = normalize(sequence) # l2 normalization\n",
    "\n",
    "    print(\"tf-idf model complete\")\n",
    "    \n",
    "    return bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 documents processed / 3\n",
      "tf-idf model complete\n",
      "[[0.    0.434 0.558 0.558 0.    0.434 0.   ]\n",
      " [0.    0.434 0.    0.    0.558 0.434 0.558]\n",
      " [0.405 0.478 0.308 0.308 0.308 0.478 0.308]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here is an example from page 238 from the machine learning book that shows this implementation I built above\n",
    "is working accordingly\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "test = np.array([\"The sun is shining\", \"The weather is sweet\", \"The sun is shining and the weather is sweet\"])\n",
    "print(tfidf(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize as tokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Now that we have a function that can process an entire sample of sequences by their tokens relative to their importance,\n",
    "we need to take care of the last preprocessing step before we can create training/testing data and do cross validation. In this\n",
    "step we will focus on cleaning up the data and removing things like stop words, tokenization of reviews, and lemmanizing reviews. \n",
    "For a brief introduction on these topics, a stop word is a word that is common in a language and is not important to the meaning of the sentence. \n",
    "For example, words like \"the\", \"a\", and \"an\" are common in English and are not important to the meaning of the sentence making them stop \n",
    "words. We need to get rid of these because one, they are not important to the meaning of the sentence and two, it's not helpful enough\n",
    "for it be a part of the features of a single review that helps depict the sentiment of the review. On the topic of tokenization, we are\n",
    "basically stripping the text from things like line breaks, blank spaces, and punctuation. This will help resolve the HTML problem a mentioned\n",
    "ealier in the project. For lemmanizing, we are reducing words to their canoical or base form (simplify). For example, we can reduce the word \n",
    "\"trained\" to \"train\". Overall, the combination of these three processes will generate reviews that hold information about the sentiment of the \n",
    "review and its general direction of which class it belongs to.\n",
    "\n",
    "To elimate stop words we will tokenize the reviews then we will strip the sequence of tokens from stop words. Then to lemmanize we will\n",
    "use the NLTK (Natural Language tool-kit) lemmatizer object and it's built in function which allow is to take the filtered tokens and simplify them into their basic\n",
    "form. Lets do this an example first to make sure we get it working before we apply it to the entire review dataframe.\n",
    "\"\"\"\n",
    "\n",
    "# helper function to get pos of word so that it's lemmatized correctly\n",
    "def pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def clean(reviews):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer() # initializing lemmatizer object\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\")) # create a set of stop words\n",
    "\n",
    "    N = len(reviews) # get the number of reviews\n",
    "\n",
    "    # apply filter to each review\n",
    "    for i, review in enumerate(reviews):\n",
    "        \n",
    "        review = re.sub(\"<[^>]*>\", \" \", review) # stripping HTML tags\n",
    "        review = re.sub(\"[\\W]+\", \" \", review.lower()) # remove all non-word characters\n",
    "\n",
    "        review = tokenizer(review) # tokenize the review\n",
    "        review = [lemmatizer.lemmatize(t, pos(t)) for t in review if t not in stop_words] # strip tokens from stop words and lemmatize\n",
    "        reviews[i] = \" \".join(review) # join the tokens back together\n",
    "\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"{i + 1} review(s) processed / {N}\")\n",
    "    \n",
    "    print(f\"{N} / {N} reviews processed\")\n",
    "\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 3 reviews processed\n",
      "['sun shin' 'weather sweet' 'sun shin weather sweet']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "As we can see we removed all stop words and lemmatized the tokens. Now we can apply this to the entire review dataframe. One thing\n",
    "I will mention is the lemmatization process is not perfect (shining -> shin x shine). Although as long as it remains consistent for the \n",
    "majority of the reviews, it should not affect the importance of tokens relative to the sentiment of the review label.\n",
    "\"\"\"\n",
    "\n",
    "print(clean(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer as TFIDF\n",
    "\n",
    "\"\"\"\n",
    "Due to the runtime of my td-idf model, I will be using transformer provided by sklearn. It will take care of the preprocessing\n",
    "in terms of turning our bag of words into a tf-idf matrix.\n",
    "\"\"\"\n",
    "\n",
    "# quick data preprocessing \n",
    "def preprocess(reviews):\n",
    "    counts = CountVectorizer()\n",
    "    transformer = TFIDF()\n",
    "    reviews = clean(reviews)\n",
    "    reviews = transformer.fit_transform(counts.fit_transform(reviews)).toarray()\n",
    "    return np.array(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocessing the training data\n",
    "X = preprocess(df[\"Review\"].values)\n",
    "\n",
    "# turning sentiment labels into one hot vectors\n",
    "y = np.array([1 if i == \"positive\" else 0 for i in df[\"Sentiment\"].values])\n",
    "\n",
    "# splitting training & testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\"\"\"\n",
    "With the data ready to go, lets build our logisite model. We can easily do this using scikit-learns LogisticRegression class. Before\n",
    "I begin peforming corss-validation on the model, I will first try it on our training/testing splits then i will progress to cross\n",
    "validation over 5 folds on the entire dataset. From there, we will be able to see the results and analyze the model. I will only iterate\n",
    "the dataset 10 times seeing 40,000 reviews per iteration should be enough for our model to learn parameters.\n",
    "\"\"\"\n",
    "\n",
    "clf = LogisticRegression(max_iter=10) # initializing logistic regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train) # fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Accuracy: 87.91%\n"
     ]
    }
   ],
   "source": [
    "acc = clf.score(x_test, y_test) # testing the model\n",
    "print(f\"Base Accuracy: {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 87.92%\n",
      "Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 88.79%\n",
      "Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 87.69%\n",
      "Iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 88.58%\n",
      "Iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 88.77000000000001%\n",
      "Average accuracy across 5 folds: 88.35000000000001%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now that we have a basic understanding on how are model peforms and a general idea of the optimal hyperparameters, we can\n",
    "now begin cross validation. For this project, I will be using the KFold cross validation method. On the entire dataset, I will\n",
    "be using 5 folds and will be declaring the accuracy of our model based on the average over the 5 folds.\n",
    "\"\"\"\n",
    "\n",
    "clf = LogisticRegression(max_iter=10)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "total_accuracy = 0\n",
    "iter_ = 1\n",
    "for train, test in kf.split(X):\n",
    "    print(f\"Iteration {iter_}\")\n",
    "    clf.fit(X[train], y[train])\n",
    "    acc = clf.score(X[test], y[test])\n",
    "    total_accuracy += acc\n",
    "    print(f\"Model Accuracy: {acc*100}%\")\n",
    "    iter_ += 1\n",
    "\n",
    "\n",
    "print(f\"Average accuracy across 5 folds: {np.array(total_accuracy / 5 * 100)}%\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[22418  2582]\n",
      " [ 2042 22958]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\"\"\"\n",
    "Before I conclude, let's look at the confusion matrix from the logistic model we built.\n",
    "\n",
    "It doesn't look like it's a perfect model, but it did a solid job of predicting the sentiment of\n",
    "reviews correctly.\n",
    "\"\"\"\n",
    "\n",
    "pred = clf.predict(X)\n",
    "cm = confusion_matrix(y, pred)\n",
    "print(f\"Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After doing cross validation we can see our model is not that bad (roughly 88%). With some tweaks and improvments we can probably get a better\n",
    "accuracy. Something I thought about doing during the project was ensebmling the model and using it to predict the sentiment of a new review\n",
    "or using a svm model to do sentiment analysis. Maybe random forest would do better, although I will leave this project as a learning exercise \n",
    "for the next project. As a whole, it is crucial to understand your data better than your model because often times it depicts how well your model \n",
    "peforms. I believe I did a good job in terms of preprocessing the data and building the model, although there is always areas to improve. \n",
    "I wonder if I could've gotten better results with a different preporcessing method or a different model entirely.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in the Data...\n",
      "5000 review(s) processed / 50000\n",
      "10000 review(s) processed / 50000\n",
      "15000 review(s) processed / 50000\n",
      "20000 review(s) processed / 50000\n",
      "25000 review(s) processed / 50000\n",
      "30000 review(s) processed / 50000\n",
      "35000 review(s) processed / 50000\n",
      "40000 review(s) processed / 50000\n",
      "45000 review(s) processed / 50000\n",
      "50000 review(s) processed / 50000\n",
      "50000 / 50000 reviews processed\n",
      "Data Loaded...\n"
     ]
    }
   ],
   "source": [
    "import preprocessing as pp\n",
    "import process_functions as pf\n",
    "import pickle\n",
    "\n",
    "\n",
    "path = pf.path\n",
    "\n",
    "X = pp.features\n",
    "y = pp.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and transformer to disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=10)\n",
    "model.fit(X, y)\n",
    "print(f\"Saving model and transformer to disk...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(f\"{path}/model/model.pkl\", \"wb\"))\n",
    "pickle.dump(pp.transformer, open(f\"{path}/model/transformer.obj\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63847a374abcb3b9a1a98c0eeba015d87a63f2412b9d7a6f458ed5afc164c9c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
